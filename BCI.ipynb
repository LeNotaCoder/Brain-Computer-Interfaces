{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvDBY6H60o9m",
        "outputId": "4bff8525-a41f-436f-fb66-ac2097bb7a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.12/dist-packages (1.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tswmOpILW79S",
        "outputId": "9471c770-af8c-4eb0-a1ae-ea8872c73c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rWQM8IcJNLjT"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import numpy as np\n",
        "import pywt\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn, optim\n",
        "import os\n",
        "import torchvision.models as models\n",
        "\n",
        "# ---------------- EEG Preprocessor ----------------\n",
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "class EEGPreprocessor:\n",
        "    def __init__(self):\n",
        "        print(\"[INIT] EEGPreprocessor initialized\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data(gdf_file, eeg_channels=1, trial_duration_sec=8):\n",
        "        # Load raw data\n",
        "        raw = mne.io.read_raw_gdf(gdf_file, preload=True)\n",
        "        sfreq = raw.info['sfreq']\n",
        "\n",
        "        # Extract events\n",
        "        events, event_id = mne.events_from_annotations(raw)\n",
        "        mi_event_codes = [event_id['769'], event_id['770'], event_id['771'], event_id['772']]\n",
        "        artifact_code = event_id.get('1023', None)\n",
        "\n",
        "        samples_per_trial = int(trial_duration_sec * sfreq)\n",
        "\n",
        "        trials = []\n",
        "        labels = []\n",
        "\n",
        "        for sample_idx, _, code in events:\n",
        "            if code not in mi_event_codes or code == artifact_code:\n",
        "                continue\n",
        "\n",
        "            data = raw.get_data(start=sample_idx, stop=sample_idx + samples_per_trial)[:eeg_channels, :]\n",
        "            if data.shape[1] != samples_per_trial or np.isnan(data).any():\n",
        "                continue\n",
        "\n",
        "            trials.append(data[:, 500:1501])  # full trial\n",
        "            labels.append(mi_event_codes.index(code))\n",
        "\n",
        "        trials = np.array(trials)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        print(f\"[LOAD] Loaded {trials.shape[0]} trials with shape {trials.shape[1:]}\")\n",
        "        return trials, labels\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_windows(trials_2_4s, labels, n_windows=5, window_samples=200):\n",
        "        n_trials, n_channels, n_samples = trials_2_4s.shape\n",
        "\n",
        "        if n_samples < n_windows * window_samples:\n",
        "            raise ValueError(f\"Not enough samples per trial ({n_samples}) for {n_windows} windows of {window_samples} samples each.\")\n",
        "\n",
        "        windowed_data = []\n",
        "        windowed_labels = []\n",
        "\n",
        "        for trial_idx in range(n_trials):\n",
        "            data = []\n",
        "            for i in range(n_windows):\n",
        "                start = i * window_samples\n",
        "                end = start + window_samples\n",
        "                data.append(trials_2_4s[trial_idx, :, start:end])\n",
        "            windowed_data.append(np.array(data))\n",
        "            windowed_labels.append(labels[trial_idx])\n",
        "\n",
        "        print(f\"[WINDOW] Split into {n_windows} windows per trial.\")\n",
        "        print(f\"[WINDOW] Windowed data shape: {len(windowed_data)}, Windowed labels shape: {len(windowed_labels)}\")\n",
        "\n",
        "        return windowed_data, windowed_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- Feature Extractor ----------------\n",
        "class FeatureExtractor:\n",
        "    def __init__(self):\n",
        "        print(\"[INIT] Loading frozen ResNet50 as feature extractor...\")\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        # Remove classification head\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.eval()\n",
        "        print(\"[INIT] ResNet50 feature extractor ready\")\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_scalogram_short(eeg_signal, sfreq=100, freqs=np.linspace(8, 30, 40), n_cycles=3, output_size=(224, 224)):\n",
        "        \"\"\"\n",
        "        Convert a 1-channel short EEG signal (~200 samples) to a Morlet wavelet scalogram.\n",
        "        \"\"\"\n",
        "        if eeg_signal.ndim != 2 or eeg_signal.shape[0] != 1:\n",
        "            raise ValueError(\"eeg_signal must have shape (1, n_samples)\")\n",
        "\n",
        "        eeg_batch = eeg_signal[np.newaxis, :, :]  # shape (1, 1, n_samples)\n",
        "\n",
        "        tfr = mne.time_frequency.tfr_array_morlet(\n",
        "            eeg_batch, sfreq=sfreq, freqs=freqs, n_cycles=n_cycles, output='power'\n",
        "        )  # shape (1, 1, n_freqs, n_times)\n",
        "\n",
        "        scalogram = tfr[0, 0]  # shape (n_freqs, n_times)\n",
        "\n",
        "        # Normalize to 0-1\n",
        "        scalogram -= scalogram.min()\n",
        "        scalogram /= scalogram.max() + 1e-6\n",
        "\n",
        "        # Resize to desired output size\n",
        "        from skimage.transform import resize\n",
        "        scalogram_resized = resize(scalogram, output_size, mode='reflect', anti_aliasing=True)\n",
        "\n",
        "        return scalogram_resized\n",
        "\n",
        "    def extract_features(self, scalogram):\n",
        "        \"\"\"\n",
        "        Instance method: extract features using the frozen ResNet50.\n",
        "        \"\"\"\n",
        "        img = torch.tensor(scalogram, dtype=torch.float32).unsqueeze(0).repeat(3, 1, 1)  # 3 channels\n",
        "        img = img.unsqueeze(0)  # add batch dimension\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat = self.model(img)\n",
        "\n",
        "        return feat.flatten().cpu().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_II9Zh2AqxhY",
        "outputId": "63cf0702-891d-46b1-b8ed-0a4d85b4dea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT] EEGPreprocessor initialized\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BCICIV_2a_gdf/A01T.gdf...\n",
            "GDF file detected\n",
            "Setting channel info structure...\n",
            "Could not determine channel type of the following channels, they will be set as EEG:\n",
            "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 672527  =      0.000 ...  2690.108 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('769'), np.str_('770'), np.str_('771'), np.str_('772')]\n",
            "[LOAD] Loaded 287 trials with shape (1, 1001)\n",
            "[WINDOW] Split into 5 windows per trial.\n",
            "[WINDOW] Windowed data shape: 287, Windowed labels shape: 287\n",
            "[INIT] Loading frozen ResNet50 as feature extractor...\n",
            "[INIT] ResNet50 feature extractor ready\n",
            "Number: 0\n",
            "Number: 50\n",
            "Number: 100\n",
            "Number: 150\n",
            "Number: 200\n",
            "Number: 250\n",
            "Final dataset shape: (287, 10240)\n",
            "Final labels shape: (0,)\n",
            "[INIT] EEGPreprocessor initialized\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BCICIV_2a_gdf/A02T.gdf...\n",
            "GDF file detected\n",
            "Setting channel info structure...\n",
            "Could not determine channel type of the following channels, they will be set as EEG:\n",
            "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 677168  =      0.000 ...  2708.672 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('769'), np.str_('770'), np.str_('771'), np.str_('772')]\n",
            "[LOAD] Loaded 287 trials with shape (1, 1001)\n",
            "[WINDOW] Split into 5 windows per trial.\n",
            "[WINDOW] Windowed data shape: 287, Windowed labels shape: 287\n",
            "[INIT] Loading frozen ResNet50 as feature extractor...\n",
            "[INIT] ResNet50 feature extractor ready\n",
            "Number: 0\n",
            "Number: 50\n",
            "Number: 100\n",
            "Number: 150\n",
            "Number: 200\n",
            "Number: 250\n",
            "Final dataset shape: (287, 10240)\n",
            "Final labels shape: (0,)\n"
          ]
        }
      ],
      "source": [
        "def get_data(path):\n",
        "    pre = EEGPreprocessor()\n",
        "    data, labels = pre.load_data(path)\n",
        "    windowed_data, windowed_labels = pre.split_into_windows(data, labels, n_windows=5, window_samples=200)\n",
        "    fe = FeatureExtractor()\n",
        "    dataset = []\n",
        "    true_labels = []\n",
        "\n",
        "    for i, window in enumerate(windowed_data):\n",
        "        channel_feats = []\n",
        "\n",
        "        for ch in range(5):\n",
        "            epoch = window[ch]\n",
        "            scalogram = fe.compute_scalogram_short(epoch)\n",
        "            feature = fe.extract_features(scalogram)\n",
        "            channel_feats.append(feature)\n",
        "\n",
        "        # Concatenate features from all channels → (n_channels * 2048,)\n",
        "        window_features = np.concatenate(channel_feats)\n",
        "\n",
        "        dataset.append(window_features)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Number: {i}\")\n",
        "\n",
        "    dataset = np.array(dataset)\n",
        "    true_labels = np.array(true_labels)\n",
        "    dataset_reshaped = dataset.reshape(287, 5, 2048)\n",
        "\n",
        "    print(\"Final dataset shape:\", dataset.shape)\n",
        "    print(\"Final labels shape:\", true_labels.shape)\n",
        "\n",
        "    return dataset_reshaped, true_labels\n",
        "\n",
        "train_data, train_labels = get_data('/content/drive/MyDrive/BCICIV_2a_gdf/A01T.gdf')\n",
        "test_data, test_labels = get_data('/content/drive/MyDrive/BCICIV_2a_gdf/A02T.gdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fevbnXbySrM",
        "outputId": "21b1f891-4b01-4e9f-e291-a45f2f28e9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape:  (574, 5, 2048)\n",
            "Labels shape:  (574,)\n"
          ]
        }
      ],
      "source": [
        "all_data = np.concatenate((train_data, test_data), axis=0)\n",
        "all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "print(\"Dataset shape: \", all_data.shape)\n",
        "print(\"Labels shape: \", all_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Crt_3ABEpHq7"
      },
      "outputs": [],
      "source": [
        "class EEGModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EEGModel, self).__init__()\n",
        "        print(\"[MODEL INIT] Initializing LSTM model...\")\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, 100)   # First FC layer with 100 neurons\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(100, 4)             # Final output layer with 4 neurons\n",
        "        self.softmax = nn.Softmax(dim=1)         # Softmax across the class dimension\n",
        "\n",
        "        print(\"[MODEL INIT] LSTM model ready\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]       # Last time-step output\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.softmax(out)   # Convert logits to probabilities\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InceptionBlock1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionBlock1D, self).__init__()\n",
        "        assert out_channels % 4 == 0, \"out_channels must be divisible by 4\"\n",
        "        branch_channels = out_channels // 4\n",
        "        \n",
        "        # 1x1 convolution branch\n",
        "        self.branch1 = nn.Conv1d(in_channels, branch_channels, kernel_size=1)\n",
        "        \n",
        "        # 1x1 -> 3x1 convolution branch\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, branch_channels, kernel_size=1),\n",
        "            nn.Conv1d(branch_channels, branch_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "        \n",
        "        # 1x1 -> 5x1 convolution branch\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, branch_channels, kernel_size=1),\n",
        "            nn.Conv1d(branch_channels, branch_channels, kernel_size=5, padding=2)\n",
        "        )\n",
        "        \n",
        "        # MaxPool -> 1x1 convolution branch\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv1d(in_channels, branch_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        b4 = self.branch4(x)\n",
        "        out = torch.cat([b1, b2, b3, b4], dim=1)  # Concatenate along channel dimension\n",
        "        return out\n",
        "\n",
        "\n",
        "class EEGInceptionModel(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=4):\n",
        "        super(EEGInceptionModel, self).__init__()\n",
        "        print(\"[MODEL INIT] Initializing Inception EEG model...\")\n",
        "\n",
        "        self.inception1 = InceptionBlock1D(in_channels, 64)\n",
        "        self.inception2 = InceptionBlock1D(64, 128)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        print(\"[MODEL INIT] Inception EEG model ready\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Expect x of shape (batch, channels, time)\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = self.pool(x).squeeze(-1)  # shape: (batch, channels)\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleEEGCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(SimpleEEGCNN, self).__init__()\n",
        "\n",
        "        # ---- Layer 1 ----\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,   # grayscale EEG image\n",
        "            out_channels=5,  # 5 kernels\n",
        "            kernel_size=5\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # ---- Layer 2 ----\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=5,\n",
        "            out_channels=10,  # doubled from 5 → 10\n",
        "            kernel_size=5\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # ---- Fully Connected ----\n",
        "        # *You must update the in_features based on image size*\n",
        "        self.fc1 = nn.Linear(10 * 53 * 53, 128)  \n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "rgtfLGcVppVG"
      },
      "outputs": [],
      "source": [
        "class EEGTrainer:\n",
        "    def __init__(self, model, lr=0.001, epochs=10):\n",
        "        print(f\"[TRAINER INIT] lr={lr}, epochs={epochs}\")\n",
        "        self.model = model\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        print(\"[TRAIN] Starting training...\")\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for X, y in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(X)\n",
        "                loss = self.criterion(outputs, y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            print(f\"[EPOCH {epoch+1}/{self.epochs}] Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        print(\"[VALIDATE] Running validation...\")\n",
        "        self.model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                outputs = self.model(X)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "        print(f\"[VALIDATE] Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"[SAVE] Model saved to {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v0QQY79iOB5",
        "outputId": "4a0dc36c-9c0c-4325-ef2b-816e73435bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DATA] Combined data shape: torch.Size([574, 5, 2048]), Labels shape: torch.Size([574])\n",
            "[MODEL INIT] Initializing LSTM model...\n",
            "[MODEL INIT] LSTM model ready\n",
            "[TRAINER INIT] lr=0.001, epochs=100\n",
            "[TRAIN] Starting training...\n",
            "[EPOCH 1/100] Loss: 1.3899\n",
            "[EPOCH 2/100] Loss: 1.3869\n",
            "[EPOCH 3/100] Loss: 1.3869\n",
            "[EPOCH 4/100] Loss: 1.3870\n",
            "[EPOCH 5/100] Loss: 1.3864\n",
            "[EPOCH 6/100] Loss: 1.3857\n",
            "[EPOCH 7/100] Loss: 1.3849\n",
            "[EPOCH 8/100] Loss: 1.3839\n",
            "[EPOCH 9/100] Loss: 1.3831\n",
            "[EPOCH 10/100] Loss: 1.3831\n",
            "[EPOCH 11/100] Loss: 1.3822\n",
            "[EPOCH 12/100] Loss: 1.3822\n",
            "[EPOCH 13/100] Loss: 1.3826\n",
            "[EPOCH 14/100] Loss: 1.3824\n",
            "[EPOCH 15/100] Loss: 1.3825\n",
            "[EPOCH 16/100] Loss: 1.3820\n",
            "[EPOCH 17/100] Loss: 1.3818\n",
            "[EPOCH 18/100] Loss: 1.3813\n",
            "[EPOCH 19/100] Loss: 1.3819\n",
            "[EPOCH 20/100] Loss: 1.3820\n",
            "[EPOCH 21/100] Loss: 1.3820\n",
            "[EPOCH 22/100] Loss: 1.3818\n",
            "[EPOCH 23/100] Loss: 1.3818\n",
            "[EPOCH 24/100] Loss: 1.3818\n",
            "[EPOCH 25/100] Loss: 1.3817\n",
            "[EPOCH 26/100] Loss: 1.3818\n",
            "[EPOCH 27/100] Loss: 1.3818\n",
            "[EPOCH 28/100] Loss: 1.3817\n",
            "[EPOCH 29/100] Loss: 1.3821\n",
            "[EPOCH 30/100] Loss: 1.3816\n",
            "[EPOCH 31/100] Loss: 1.3819\n",
            "[EPOCH 32/100] Loss: 1.3817\n",
            "[EPOCH 33/100] Loss: 1.3817\n",
            "[EPOCH 34/100] Loss: 1.3821\n",
            "[EPOCH 35/100] Loss: 1.3818\n",
            "[EPOCH 36/100] Loss: 1.3811\n",
            "[EPOCH 37/100] Loss: 1.3817\n",
            "[EPOCH 38/100] Loss: 1.3820\n",
            "[EPOCH 39/100] Loss: 1.3817\n",
            "[EPOCH 40/100] Loss: 1.3817\n",
            "[EPOCH 41/100] Loss: 1.3811\n",
            "[EPOCH 42/100] Loss: 1.3818\n",
            "[EPOCH 43/100] Loss: 1.3815\n",
            "[EPOCH 44/100] Loss: 1.3817\n",
            "[EPOCH 45/100] Loss: 1.3816\n",
            "[EPOCH 46/100] Loss: 1.3817\n",
            "[EPOCH 47/100] Loss: 1.3816\n",
            "[EPOCH 48/100] Loss: 1.3817\n",
            "[EPOCH 49/100] Loss: 1.3817\n",
            "[EPOCH 50/100] Loss: 1.3817\n",
            "[EPOCH 51/100] Loss: 1.3821\n",
            "[EPOCH 52/100] Loss: 1.3816\n",
            "[EPOCH 53/100] Loss: 1.3820\n",
            "[EPOCH 54/100] Loss: 1.3818\n",
            "[EPOCH 55/100] Loss: 1.3817\n",
            "[EPOCH 56/100] Loss: 1.3816\n",
            "[EPOCH 57/100] Loss: 1.3817\n",
            "[EPOCH 58/100] Loss: 1.3816\n",
            "[EPOCH 59/100] Loss: 1.3817\n",
            "[EPOCH 60/100] Loss: 1.3816\n",
            "[EPOCH 61/100] Loss: 1.3819\n",
            "[EPOCH 62/100] Loss: 1.3816\n",
            "[EPOCH 63/100] Loss: 1.3816\n",
            "[EPOCH 64/100] Loss: 1.3816\n",
            "[EPOCH 65/100] Loss: 1.3821\n",
            "[EPOCH 66/100] Loss: 1.3815\n",
            "[EPOCH 67/100] Loss: 1.3816\n",
            "[EPOCH 68/100] Loss: 1.3817\n",
            "[EPOCH 69/100] Loss: 1.3816\n",
            "[EPOCH 70/100] Loss: 1.3816\n",
            "[EPOCH 71/100] Loss: 1.3817\n",
            "[EPOCH 72/100] Loss: 1.3816\n",
            "[EPOCH 73/100] Loss: 1.3817\n",
            "[EPOCH 74/100] Loss: 1.3816\n",
            "[EPOCH 75/100] Loss: 1.3815\n",
            "[EPOCH 76/100] Loss: 1.3815\n",
            "[EPOCH 77/100] Loss: 1.3816\n",
            "[EPOCH 78/100] Loss: 1.3816\n",
            "[EPOCH 79/100] Loss: 1.3816\n",
            "[EPOCH 80/100] Loss: 1.3816\n",
            "[EPOCH 81/100] Loss: 1.3809\n",
            "[EPOCH 82/100] Loss: 1.3817\n",
            "[EPOCH 83/100] Loss: 1.3817\n",
            "[EPOCH 84/100] Loss: 1.3816\n",
            "[EPOCH 85/100] Loss: 1.3816\n",
            "[EPOCH 86/100] Loss: 1.3815\n",
            "[EPOCH 87/100] Loss: 1.3815\n",
            "[EPOCH 88/100] Loss: 1.3815\n",
            "[EPOCH 89/100] Loss: 1.3815\n",
            "[EPOCH 90/100] Loss: 1.3816\n",
            "[EPOCH 91/100] Loss: 1.3809\n",
            "[EPOCH 92/100] Loss: 1.3816\n",
            "[EPOCH 93/100] Loss: 1.3810\n",
            "[EPOCH 94/100] Loss: 1.3816\n",
            "[EPOCH 95/100] Loss: 1.3817\n",
            "[EPOCH 96/100] Loss: 1.3816\n",
            "[EPOCH 97/100] Loss: 1.3815\n",
            "[EPOCH 98/100] Loss: 1.3815\n",
            "[EPOCH 99/100] Loss: 1.3816\n",
            "[EPOCH 100/100] Loss: 1.3816\n",
            "[VALIDATE] Running validation...\n",
            "[VALIDATE] Accuracy: 26.09%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==============================\n",
        "# DATA PREPARATION\n",
        "# ==============================\n",
        "\n",
        "\n",
        "X = torch.tensor(all_data, dtype=torch.float32)\n",
        "y = torch.tensor(all_labels, dtype=torch.long)\n",
        "print(f\"[DATA] Combined data shape: {X.shape}, Labels shape: {y.shape}\")\n",
        "\n",
        "# Split into train/test sets (80% train, 20% test)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y  # stratify keeps class balance\n",
        ")\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# ==============================\n",
        "# MODEL TRAINING\n",
        "# ==============================\n",
        "model = EEGModel(input_size=2048, hidden_size=64)\n",
        "trainer = EEGTrainer(model, lr=0.001, epochs=100)\n",
        "trainer.train(train_loader)\n",
        "trainer.validate(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWlAYDwGkeHo",
        "outputId": "196d9214-a361-45ae-f4ca-5b84cc499e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PIPELINE COMPLETE] Model saved to LSTM_MI_RES.pth\n"
          ]
        }
      ],
      "source": [
        "model_path=\"LSTM_MI_RES.pth\"\n",
        "\n",
        "print(f\"[PIPELINE COMPLETE] Model saved to {model_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
